# DiT (Diffusion Transformer) è®­ç»ƒæŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨ Lightning-Hydra æ¡†æ¶è®­ç»ƒ DiT (Diffusion Transformer) æ¨¡å‹ï¼Œè¿™æ˜¯ RAE é¡¹ç›®çš„ç¬¬äºŒé˜¶æ®µï¼ˆStage 2ï¼‰ã€‚

## ğŸ“‹ ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [å‡†å¤‡å·¥ä½œ](#å‡†å¤‡å·¥ä½œ)
3. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
4. [è®­ç»ƒ DiT æ¨¡å‹](#è®­ç»ƒ-dit-æ¨¡å‹)
5. [é‡‡æ ·ç”Ÿæˆ](#é‡‡æ ·ç”Ÿæˆ)
6. [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
7. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## æ¦‚è¿°

### ä»€ä¹ˆæ˜¯ DiTï¼Ÿ

DiT (Diffusion Transformer) æ˜¯ä¸€ç§åŸºäº Transformer çš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºé«˜è´¨é‡å›¾åƒç”Ÿæˆã€‚åœ¨æœ¬æ¡†æ¶ä¸­ï¼ŒDiT æ¨¡å‹åœ¨ RAE (Reconstruction Autoencoder) çš„æ½œç©ºé—´ä¸­è¿›è¡Œè®­ç»ƒï¼Œå®ç°é«˜æ•ˆçš„é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆã€‚

### è®­ç»ƒæµç¨‹

```
Stage 1 (RAE): è®­ç»ƒè‡ªç¼–ç å™¨
    â†’ å›¾åƒ â†’ RAE Encoder â†’ æ½œåœ¨è¡¨ç¤º (Latent)
    â†’ æ½œåœ¨è¡¨ç¤º â†’ RAE Decoder â†’ é‡å»ºå›¾åƒ

Stage 2 (DiT): è®­ç»ƒæ‰©æ•£æ¨¡å‹
    â†’ éšæœºå™ªå£° â†’ DiT æ¨¡å‹ â†’ æ½œåœ¨è¡¨ç¤º
    â†’ æ½œåœ¨è¡¨ç¤º â†’ RAE Decoder â†’ ç”Ÿæˆå›¾åƒ
```

### å…³é”®ç‰¹æ€§

- âœ… **æ½œç©ºé—´è®­ç»ƒ**: åœ¨ RAE çš„æ½œç©ºé—´ä¸­è®­ç»ƒï¼Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬
- âœ… **åˆ†ç±»å™¨æ— å…³å¼•å¯¼ (CFG)**: æ”¯æŒæ¡ä»¶ç”Ÿæˆå’Œé«˜è´¨é‡çš„å›¾åƒç”Ÿæˆ
- âœ… **EMA æ›´æ–°**: ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡æé«˜ç”Ÿæˆè´¨é‡
- âœ… **çµæ´»é…ç½®**: åŸºäº Hydra çš„é…ç½®ç³»ç»Ÿï¼Œæ˜“äºå®éªŒ
- âœ… **åˆ†å¸ƒå¼è®­ç»ƒ**: æ”¯æŒ DDP å¤š GPU è®­ç»ƒ

---

## å‡†å¤‡å·¥ä½œ

### 1. å…ˆå†³æ¡ä»¶

åœ¨è®­ç»ƒ DiT æ¨¡å‹ä¹‹å‰ï¼Œéœ€è¦ï¼š

- âœ… å®Œæˆ RAE (Stage 1) è®­ç»ƒ
- âœ… è·å¾— RAE æ£€æŸ¥ç‚¹æ–‡ä»¶
- âœ… å‡†å¤‡å¥½è®­ç»ƒæ•°æ®é›†ï¼ˆImageNet æˆ–å…¶ä»–ï¼‰

### 2. å®‰è£…ä¾èµ–

```bash
# ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–
pip install -r requirements.txt

# æˆ–ä½¿ç”¨ conda
conda env create -f environment.yaml
conda activate rae-train
```

### 3. å‡†å¤‡ RAE æ£€æŸ¥ç‚¹

ç¡®ä¿ä½ å·²ç»è®­ç»ƒäº† RAE æ¨¡å‹å¹¶è·å¾—æ£€æŸ¥ç‚¹ï¼š

```bash
# RAE æ£€æŸ¥ç‚¹é€šå¸¸ä½äº
logs/train/runs/YYYY-MM-DD/HH-MM-SS/checkpoints/
â”œâ”€â”€ last.ckpt
â”œâ”€â”€ epoch=X-step=Y.ckpt
â””â”€â”€ ...
```

---

## å¿«é€Ÿå¼€å§‹

### å°è§„æ¨¡æµ‹è¯•

åœ¨å®Œæ•´è®­ç»ƒä¹‹å‰ï¼Œå»ºè®®å…ˆè¿è¡Œå°è§„æ¨¡æµ‹è¯•ï¼š

```bash
# CPU æµ‹è¯•ï¼ˆéªŒè¯ä»£ç ï¼‰
python src/train.py experiment=dit_dummy

# GPU æµ‹è¯•ï¼ˆéªŒè¯ GPU æ”¯æŒï¼‰
python src/train.py experiment=dit_dummy trainer=gpu
```

### å®Œæ•´è®­ç»ƒ

```bash
# ä½¿ç”¨é¢„è®­ç»ƒçš„ RAE æ£€æŸ¥ç‚¹è®­ç»ƒ DiT
python src/train.py experiment=dit_dino \
    model.rae.encoder_checkpoint_path=/path/to/rae_checkpoint.ckpt

# è‡ªå®šä¹‰è®­ç»ƒå‚æ•°
python src/train.py experiment=dit_dino \
    model.rae.encoder_checkpoint_path=/path/to/rae_checkpoint.ckpt \
    data.data_dir=/path/to/imagenet_hf \
    data.batch_size=32 \
    model.dit_module.learning_rate=1e-4
```

---

## è®­ç»ƒ DiT æ¨¡å‹

### å• GPU è®­ç»ƒ

```bash
# åŸºç¡€è®­ç»ƒå‘½ä»¤
python src/train.py experiment=dit_dino \
    model.rae.encoder_checkpoint_path=/path/to/rae_checkpoint.ckpt

# ä½¿ç”¨è¾ƒå°çš„ batch size
python src/train.py experiment=dit_dino \
    model.rae.encoder_checkpoint_path=/path/to/rae_checkpoint.ckpt \
    data.batch_size=16
```

### å¤š GPU DDP è®­ç»ƒ

```bash
# ä½¿ç”¨ 8 ä¸ª GPUï¼ˆæ¨èé…ç½®ï¼‰
python src/train.py experiment=dit_dino \
    model.rae.encoder_checkpoint_path=/path/to/rae_checkpoint.ckpt \
    trainer=ddp

# ä½¿ç”¨ 4 ä¸ª GPU
python src/train.py experiment=dit_dino \
    model.rae.encoder_checkpoint_path=/path/to/rae_checkpoint.ckpt \
    trainer=ddp \
    trainer.devices=4 \
    data.batch_size=64

# ä½¿ç”¨ torchrun å¯åŠ¨
torchrun --nproc_per_node=8 src/train.py experiment=dit_dino \
    model.rae.encoder_checkpoint_path=/path/to/rae_checkpoint.ckpt
```

### æ¢å¤è®­ç»ƒ

```bash
# ä»æœ€æ–°æ£€æŸ¥ç‚¹æ¢å¤
python src/train.py experiment=dit_dino \
    model.rae.encoder_checkpoint_path=/path/to/rae_checkpoint.ckpt \
    ckpt_path="last"

# ä»æŒ‡å®šæ£€æŸ¥ç‚¹æ¢å¤
python src/train.py experiment=dit_dino \
    model.rae.encoder_checkpoint_path=/path/to/rae_checkpoint.ckpt \
    ckpt_path="/path/to/dit_checkpoint.ckpt"
```

---

## é‡‡æ ·ç”Ÿæˆ

### åŸºç¡€é‡‡æ ·

```bash
# ç”Ÿæˆ 100 å¼ éšæœºå›¾åƒ
python src/eval_dit.py \
    --checkpoint /path/to/dit_checkpoint.ckpt \
    --num_samples 100 \
    --output_dir outputs/samples

# ä½¿ç”¨åˆ†ç±»å™¨æ— å…³å¼•å¯¼ (CFG)
python src/eval_dit.py \
    --checkpoint /path/to/dit_checkpoint.ckpt \
    --num_samples 100 \
    --cfg_scale 2.0 \
    --output_dir outputs/samples_cfg2

# å¢åŠ é‡‡æ ·æ­¥æ•°ï¼ˆæé«˜è´¨é‡ï¼‰
python src/eval_dit.py \
    --checkpoint /path/to/dit_checkpoint.ckpt \
    --num_samples 100 \
    --num_steps 200 \
    --output_dir outputs/samples_200steps
```

### æ¡ä»¶ç”Ÿæˆï¼ˆæŒ‡å®šç±»åˆ«ï¼‰

```bash
# ç”Ÿæˆç‰¹å®šç±»åˆ«çš„å›¾åƒ
python src/eval_dit.py \
    --checkpoint /path/to/dit_checkpoint.ckpt \
    --labels "0,1,2,3,4" \
    --output_dir outputs/samples_classes

# ä½¿ç”¨ç±»åˆ«åç§°åˆ—è¡¨
python src/eval_dit.py \
    --checkpoint /path/to/dit_checkpoint.ckpt \
    --class_list /path/to/class_names.txt \
    --num_samples 50 \
    --output_dir outputs/samples
```

### æ‰¹é‡é‡‡æ ·

```bash
# ç”Ÿæˆå¤§é‡æ ·æœ¬ç”¨äº FID è¯„ä¼°
python src/eval_dit.py \
    --checkpoint /path/to/dit_checkpoint.ckpt \
    --num_samples 10000 \
    --output_dir outputs/fid_samples \
    --num_steps 50
```

---

## é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½® (`configs/model/dit.yaml`)

```yaml
model:
  # RAE ç¼–ç å™¨é…ç½®ï¼ˆå†»ç»“ï¼‰
  rae:
    encoder_cls: 'Dinov2withNorm'
    encoder_config_path: 'facebook/dinov2-with-registers-base'
    encoder_input_size: 224
    encoder_checkpoint_path: null  # è®­ç»ƒæ—¶æŒ‡å®š
  
  # DiT/DDT æ¨¡å‹é…ç½®
  dit:
    input_size: 16  # æ½œåœ¨ç©ºé—´å°ºå¯¸ (16x16)
    patch_size: 1
    in_channels: 768  # RAE æ½œåœ¨é€šé“æ•°
    hidden_size: [1152, 2048]  # Encoder/Decoder éšè—å±‚å¤§å°
    depth: [28, 2]  # Encoder/Decoder å±‚æ•°
    num_heads: [16, 16]  # æ³¨æ„åŠ›å¤´æ•°
    mlp_ratio: 4.0
    class_dropout_prob: 0.1
    num_classes: 1000
  
  # Lightning Module é…ç½®
  dit_module:
    ema_decay: 0.9995  # EMA è¡°å‡ç‡
    learning_rate: 2.0e-4
    warmup_steps: 5000
    max_steps: 100000
    num_classes: 1000
    null_label: 1000
    latent_size: [768, 16, 16]
```

### è®­ç»ƒå™¨é…ç½® (`configs/experiment/dit_dino.yaml`)

```yaml
trainer:
  max_epochs: 1400
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  precision: 16  # æ··åˆç²¾åº¦è®­ç»ƒ
  check_val_every_n_epoch: 1
  log_every_n_steps: 100

data:
  image_size: 256
  batch_size: 32  # æ¯ä¸ª GPU çš„ batch size
  num_workers: 8
  train_split: 1.0  # ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®
```

### å¸¸ç”¨å‚æ•°è¦†ç›–

```bash
# ä¿®æ”¹å­¦ä¹ ç‡
python src/train.py experiment=dit_dino \
    model.dit_module.learning_rate=1e-4

# ä¿®æ”¹ EMA è¡°å‡ç‡
python src/train.py experiment=dit_dino \
    model.dit_module.ema_decay=0.999

# ä¿®æ”¹ batch size
python src/train.py experiment=dit_dino \
    data.batch_size=64

# ä¿®æ”¹å›¾åƒå°ºå¯¸
python src/train.py experiment=dit_dino \
    data.image_size=512

# å¯ç”¨æ¨¡å‹ç¼–è¯‘ï¼ˆPyTorch 2.0+ï¼‰
python src/train.py experiment=dit_dino \
    model.dit_module.compile=true

# ä¿®æ”¹è®­ç»ƒæ­¥æ•°
python src/train.py experiment=dit_dino \
    model.dit_module.max_steps=50000 \
    model.dit_module.warmup_steps=1000
```

---

## å¸¸è§é—®é¢˜

### 1. æ˜¾å­˜ä¸è¶³ (OOM)

**é—®é¢˜**: CUDA out of memory

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å‡å° batch size
python src/train.py experiment=dit_dino data.batch_size=8

# å‡å°å›¾åƒå°ºå¯¸
python src/train.py experiment=dit_dino data.image_size=128

# ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
python src/train.py experiment=dit_dino \
    data.batch_size=8 \
    trainer.accumulate_grad_batches=4

# ä½¿ç”¨æ›´å°çš„ DiT æ¨¡å‹
# ä¿®æ”¹ configs/experiment/dit_dummy.yaml ä¸­çš„ DiT é…ç½®
```

### 2. è®­ç»ƒé€Ÿåº¦æ…¢

**é—®é¢˜**: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å¢åŠ æ•°æ®åŠ è½½è¿›ç¨‹æ•°
python src/train.py experiment=dit_dino data.num_workers=16

# ä½¿ç”¨æ··åˆç²¾åº¦ï¼ˆé»˜è®¤å·²å¯ç”¨ï¼‰
python src/train.py experiment=dit_dino trainer.precision=16

# å¯ç”¨æ¨¡å‹ç¼–è¯‘ï¼ˆPyTorch 2.0+ï¼‰
python src/train.py experiment=dit_dino model.dit_module.compile=true

# ä½¿ç”¨æ›´å¤š GPU
python src/train.py experiment=dit_dino trainer=ddp trainer.devices=8
```

### 3. ç”Ÿæˆè´¨é‡å·®

**é—®é¢˜**: ç”Ÿæˆçš„å›¾åƒè´¨é‡ä¸å¥½

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å¢åŠ é‡‡æ ·æ­¥æ•°
python src/eval_dit.py \
    --checkpoint /path/to/checkpoint.ckpt \
    --num_steps 200

# ä½¿ç”¨åˆ†ç±»å™¨æ— å…³å¼•å¯¼
python src/eval_dit.py \
    --checkpoint /path/to/checkpoint.ckpt \
    --cfg_scale 3.0

# ç¡®ä¿ä½¿ç”¨ EMA æƒé‡
# eval_dit.py é»˜è®¤ä½¿ç”¨ ema_dit æƒé‡

# è®­ç»ƒæ›´å¤š epochs
python src/train.py experiment=dit_dino trainer.max_epochs=2000
```

### 4. RAE æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥

**é—®é¢˜**: æ— æ³•åŠ è½½ RAE æ£€æŸ¥ç‚¹

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥æ£€æŸ¥ç‚¹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
ls -l /path/to/rae_checkpoint.ckpt

# æ£€æŸ¥ RAE é…ç½®æ˜¯å¦åŒ¹é…
# ç¡®ä¿ configs/experiment/dit_dino.yaml ä¸­çš„ RAE é…ç½®ä¸è®­ç»ƒæ—¶ä¸€è‡´

# å°è¯•åŠ è½½ä¸åŒçš„ state_dict é”®
# DiTModule ä¼šè‡ªåŠ¨å°è¯•ä¸åŒçš„é”®å: "state_dict", "model", "ema"
```

### 5. æ•°æ®åŠ è½½é”™è¯¯

**é—®é¢˜**: æ•°æ®åŠ è½½å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥æ•°æ®ç›®å½•
ls -l /path/to/imagenet_hf

# æ£€æŸ¥æ•°æ®æ ¼å¼
# ç¡®ä¿ ImageNet å·²è½¬æ¢ä¸º HuggingFace æ ¼å¼

# ä½¿ç”¨æœ¬åœ°æ•°æ®é›†æ ¼å¼
python src/train.py experiment=dit_dino \
    data.use_hf_dataset=false \
    data.data_dir=/path/to/imagenet/train
```

---

## é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰ DiT æ¶æ„

```python
# åœ¨ configs/experiment/dit_dino.yaml ä¸­ä¿®æ”¹ DiT é…ç½®
dit:
  input_size: 16  # æ½œåœ¨ç©ºé—´å°ºå¯¸
  hidden_size: [1024, 1536]  # è‡ªå®šä¹‰éšè—å±‚å¤§å°
  depth: [20, 4]  # è‡ªå®šä¹‰å±‚æ•°
  num_heads: [12, 12]  # è‡ªå®šä¹‰æ³¨æ„åŠ›å¤´æ•°
  use_rope: true  # å¯ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
  use_rmsnorm: true  # å¯ç”¨ RMSNorm
  use_swiglu: true  # å¯ç”¨ SwiGLU
```

### ä½¿ç”¨ä¸åŒçš„ RAE ç¼–ç å™¨

```bash
# ä½¿ç”¨ SigLIP ç¼–ç å™¨
python src/train.py experiment=dit_dino \
    model.rae.encoder_cls='SigLIPwithNorm' \
    model.rae.encoder_config_path='google/siglip-so400m-patch14-384'

# ä½¿ç”¨ MAE ç¼–ç å™¨
python src/train.py experiment=dit_dino \
    model.rae.encoder_cls='MAEwithNorm' \
    model.rae.encoder_config_path='facebook/mae-base'
```

### åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–

```bash
# ä½¿ç”¨ NCCL åç«¯ï¼ˆå¤šæœºï¼‰
torchrun \
    --nproc_per_node=8 \
    --nnodes=2 \
    --node_rank=0 \
    --master_addr="192.168.1.1" \
    --master_port=29500 \
    src/train.py experiment=dit_dino

# ä½¿ç”¨ FSDPï¼ˆå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œï¼‰
python src/train.py experiment=dit_dino \
    trainer.strategy=fsdp
```

---

## ç›‘æ§è®­ç»ƒ

### TensorBoard

```bash
# å¯åŠ¨ TensorBoard
tensorboard --logdir logs/runs

# åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ http://localhost:6006
```

æŸ¥çœ‹çš„æŒ‡æ ‡ï¼š
- `train/loss`: è®­ç»ƒæŸå¤±
- `val/loss`: éªŒè¯æŸå¤±
- `train/lr`: å­¦ä¹ ç‡

### æ—¥å¿—ç›®å½•

```
logs/
â”œâ”€â”€ runs/
â”‚   â””â”€â”€ YYYY-MM-DD/
â”‚       â””â”€â”€ HH-MM-SS/
â”‚           â”œâ”€â”€ .hydra/
â”‚           â”‚   â””â”€â”€ config.yaml       # å®Œæ•´é…ç½®
â”‚           â”œâ”€â”€ checkpoints/          # æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚           â”‚   â”œâ”€â”€ last.ckpt         # æœ€æ–°æ£€æŸ¥ç‚¹
â”‚           â”‚   â””â”€â”€ epoch=*.ckpt      # æœ€ä½³æ£€æŸ¥ç‚¹
â”‚           â””â”€â”€ events.out.tfevents.* # TensorBoard æ—¥å¿—
```

---

## æ€§èƒ½åŸºå‡†

### è®­ç»ƒé…ç½®å»ºè®®

| åœºæ™¯ | GPU æ•°é‡ | Batch Size | å›¾åƒå°ºå¯¸ | è®­ç»ƒæ—¶é—´ (é¢„ä¼°) |
|------|----------|------------|----------|----------------|
| æµ‹è¯• | 1 | 4 | 128 | 10 åˆ†é’Ÿ |
| å°è§„æ¨¡ | 1 | 8 | 256 | 2-3 å°æ—¶ |
| ä¸­ç­‰ | 4 | 32 | 256 | 6-8 å°æ—¶ |
| å®Œæ•´ | 8 | 64 | 256 | 3-4 å¤© |

### ç”Ÿæˆè´¨é‡å»ºè®®

| é‡‡æ ·æ­¥æ•° | CFG Scale | è´¨é‡ | é€Ÿåº¦ |
|---------|-----------|------|------|
| 50 | 1.0 | åŸºç¡€ | å¿« |
| 100 | 2.0 | è‰¯å¥½ | ä¸­ç­‰ |
| 200 | 3.0 | ä¼˜ç§€ | æ…¢ |
| 250 | 4.0+ | æœ€ä½³ | å¾ˆæ…¢ |

---

## ç›¸å…³èµ„æº

- [åŸå§‹ RAE ä»“åº“](../RAE)
- [RAE è®­ç»ƒæŒ‡å—](RAE_TRAINING_GUIDE.md)
- [Linear Probing æŒ‡å—](LINEAR_PROBING_GUIDE.md)
- [DiT è®ºæ–‡](https://arxiv.org/abs/2212.09748)
- [Lightning æ–‡æ¡£](https://pytorch-lightning.readthedocs.io/)

---

## è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£æˆ–æäº¤ Issueã€‚