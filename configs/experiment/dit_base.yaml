# @package _global_

# DiT-Base Experiment Configuration
# Base DiT model for ImageNet 256x256 generation

defaults:
  - override /data: imagenet
  - override /model: dit
  - override /trainer: gpu
  - override /callbacks: dit_callbacks
  - override /logger: tensorboard

# Experiment name
task_name: "dit_base_imagenet256"
tags: ["dit", "base", "imagenet256", "stage2"]

# Data configuration
data:
  image_size: 256
  batch_size: 32  # Per GPU batch size (global_batch_size=1024 / 32 GPUs = 32)
  num_workers: 8
  train_split: 1.0
  pin_memory: true

# Model configuration - DiT-Base architecture
model:
  _target_: src.models.dit_module.DiTModule

  # RAE encoder configuration (frozen)
  rae:
    _target_: src.models.stage1.RAE
    encoder_cls: 'Dinov2withNorm'
    encoder_config_path: 'facebook/dinov2-with-registers-base'
    encoder_input_size: 224
    encoder_params:
      dinov2_path: 'facebook/dinov2-with-registers-base'
      normalize: true
    decoder_config_path: 'models/decoders/dinov2/wReg_base/ViTBase'
    pretrained_decoder_path: null  # Set to trained RAE checkpoint
    noise_tau: 0.0
    reshape_to_2d: true
    normalization_stat_path: 'models/stats/dinov2/wReg_base/imagenet1k/stat.pt'

  # DiT-Base configuration
  dit:
    _target_: src.models.stage2.DiTwDDTHead
    input_size: 16
    patch_size: 1
    in_channels: 768
    hidden_size: [768, 1536]  # DiT-Base: smaller hidden size
    depth: [12, 2]  # DiT-Base: 12 encoder blocks, 2 decoder blocks
    num_heads: [12, 16]
    mlp_ratio: 4.0
    class_dropout_prob: 0.1
    num_classes: 1000
    use_qknorm: false
    use_swiglu: true
    use_rope: true
    use_rmsnorm: true
    wo_shift: false
    use_pos_embed: true

  # DiT Module training configuration
  dit_module:
    ema_decay: 0.9995
    learning_rate: 2.0e-4
    weight_decay: 0.0
    betas: [0.9, 0.95]
    warmup_steps: 5000
    max_steps: 100000
    num_classes: 1000
    null_label: 1000
    latent_size: [768, 16, 16]
    compile: false

  # Transport configuration
  transport:
    path_type: 'Linear'
    prediction: 'velocity'
    loss_weight: null
    time_dist_type: 'logit-normal_0_1'

  # Sampler configuration
  sampler:
    mode: ODE
    sampling_method: 'euler'
    num_steps: 50
    atol: 1.0e-6
    rtol: 1.0e-3
    reverse: false

  # Guidance configuration
  guidance:
    method: 'cfg'
    scale: 1.0
    t_min: 0.0
    t_max: 1.0

# Trainer configuration
trainer:
  max_epochs: 1400
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  precision: 16
  check_val_every_n_epoch: 1
  log_every_n_steps: 100
  val_check_interval: 10000
  limit_val_batches: 1

# Callbacks
callbacks:
  model_checkpoint:
    monitor: "val/loss"
    mode: "min"
    save_top_k: 3
    save_last: true
    every_n_epochs: 10

  evaluation:
    eval_interval: 25000
    fid_num_samples: 10000

# Paths
paths:
  root_dir: ${oc.env:PROJECT_ROOT,.}
  data_dir: ${paths.root_dir}/data
  log_dir: ${paths.root_dir}/logs
  output_dir: ${hydra:runtime.cwd}/outputs

# RAE-specific settings
rae_frozen: true
encoder_checkpoint_path: null
