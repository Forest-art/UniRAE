# @package _global_

defaults:
  - override /data: imagenet
  - override /model: dit
  - override /trainer: gpu
  - override /callbacks: default
  - override /logger: tensorboard
  - override /hydra/sweeper: basic

# Experiment name
task_name: "stage2_dit_xl_dinov2"

# Data configuration
data:
  image_size: 256
  batch_size: 32  # Per GPU batch size (global_batch_size=1024 / 32 GPUs = 32)
  num_workers: 8
  train_split: 1.0  # Use all training data
  pin_memory: true

# Model configuration - DiT-XL architecture matching RAE
model:
  _target_: src.models.dit_module.DiTModule

  # RAE encoder configuration (frozen)
  rae:
    _target_: src.models.stage1.RAE
    encoder_cls: 'Dinov2withNorm'
    encoder_config_path: 'facebook/dinov2-with-registers-base'
    encoder_input_size: 224
    encoder_params:
      dinov2_path: 'facebook/dinov2-with-registers-base'
      normalize: true
    decoder_config_path: 'models/decoders/dinov2/wReg_base/ViTXL'
    pretrained_decoder_path: 'models/decoders/dinov2/wReg_base/ViTXL_n08/model.pt'
    noise_tau: 0.0
    reshape_to_2d: true
    # normalization_stat_path: 'models/stats/dinov2/wReg_base/imagenet1k/stat.pt'

  # DiT-XL configuration (matches RAE DiTDH-XL)
  dit:
    _target_: src.models.stage2.DiTwDDTHead
    input_size: 16
    patch_size: 1
    in_channels: 768
    hidden_size: [1152, 2048]  # DiT-XL encoder/decoder hidden sizes
    depth: [28, 2]  # DiT-XL encoder/decoder depth
    num_heads: [16, 16]
    mlp_ratio: 4.0
    class_dropout_prob: 0.1
    num_classes: 1000
    use_qknorm: false
    use_swiglu: true
    use_rope: true
    use_rmsnorm: true
    wo_shift: false
    use_pos_embed: true

  # DiT Module training configuration (matches RAE training settings)
  dit_module:
    ema_decay: 0.9995
    learning_rate: 2.0e-4
    weight_decay: 0.0
    betas: [0.9, 0.95]
    warmup_steps: 5000  # Approx 40 epochs at global_batch_size=1024
    max_steps: 100000  # Approx 800 epochs
    num_classes: 1000
    null_label: 1000
    latent_size: [768, 16, 16]  # [C, H, W]
    compile: false

  # Transport configuration (matches RAE)
  transport:
    path_type: 'Linear'
    prediction: 'velocity'
    loss_weight: null
    time_dist_type: 'logit-normal_0_1'

  # Sampler configuration (matches RAE)
  sampler:
    mode: ODE
    sampling_method: 'euler'
    num_steps: 50
    atol: 1.0e-6
    rtol: 1.0e-3
    reverse: false

  # Guidance configuration (matches RAE)
  guidance:
    method: 'cfg'
    scale: 1.0
    t_min: 0.0
    t_max: 1.0

# Trainer configuration (matches RAE training settings)
trainer:
  max_epochs: 1400
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1  # Adjust based on GPU memory
  precision: 16
  check_val_every_n_epoch: 1
  log_every_n_steps: 100

  # Sampling configuration
  val_check_interval: 10000  # Sample every 10000 steps
  limit_val_batches: 1

# Callbacks
callbacks:
  model_checkpoint:
    monitor: "val/loss"
    mode: "min"
    save_top_k: 3
    save_last: true
    every_n_epochs: 10

  # Evaluation callback
  evaluation:
    eval_interval: 25000  # Steps
    fid_num_samples: 10000  # Number of samples for FID computation

# Paths
paths:
  root_dir: ${oc.env:PROJECT_ROOT,.}
  data_dir: ${paths.root_dir}/data
  log_dir: ${paths.root_dir}/logs
  output_dir: ${hydra:runtime.cwd}/outputs

# Hydra configuration
hydra:
  run:
    dir: ${paths.log_dir}/runs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${paths.log_dir}/multiruns/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  mode: MULTIRUN
  verbose: false

# Additional RAE-specific settings
# Encoder should be frozen during stage2 training
rae_frozen: true
encoder_checkpoint_path: null  # Set to path of trained RAE checkpoint
