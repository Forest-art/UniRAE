# @package _global_
# RAE DINO experiment configuration
# Based on RAE/configs/stage1/training/DINOv2-B_decXL.yaml

defaults:
  - override /data: imagenet
  - override /model: rae
  - override /callbacks: rae
  - override /trainer: default
  - override /logger: tensorboard

# Experiment tags
tags: ["rae", "dino", "stage1"]

# Random seed
seed: 12345

# Trainer configuration
# Original: epochs=16
trainer:
  max_epochs: 16
  precision: 16  # Use mixed precision training (fp16)
  accumulate_grad_batches: 1
  gradient_clip_val: null
  val_check_interval: 1.0  # Run validation every epoch (needed for rFID callback)
  check_val_every_n_epoch: 1

# Data configuration
# Original: global_batch_size=512, num_workers=8
# Adjust batch_size based on number of GPUs: 512 / num_gpus
data:
  batch_size: 128  # For 4 GPUs: 512/4=128. Adjust based on your setup
  num_workers: 8
