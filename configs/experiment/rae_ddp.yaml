# RAE DDP Training Configuration
# Uses Distributed Data Parallel (DDP) for multi-GPU training

# @package _global_

defaults:
  - rae_dino
  - override /trainer: ddp

# Override trainer settings for DDP
trainer:
  max_epochs: 100
  accumulate_grad_batches: 1

# Adjust batch size for distributed training
data:
  batch_size: 32  # Per-GPU batch size
  num_workers: 8  # More workers for better data loading

# Model settings
model:
  use_ema: true
  ema_decay: 0.9999