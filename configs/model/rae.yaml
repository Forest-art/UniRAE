# RAE model configuration

_target_: src.models.rae_module.RAELitModule

# RAE configuration
encoder_cls: 'Dinov2withNorm'
encoder_config_path: 'facebook/dinov2-with-registers-base'
encoder_input_size: 224
encoder_params:
  dinov2_path: 'facebook/dinov2-with-registers-base'
  normalize: true

decoder_config_path: '${paths.root_dir}/configs/decoder/ViTXL/config.json'
decoder_patch_size: 16
pretrained_decoder_path: null
noise_tau: 0.8
reshape_to_2d: true
normalization_stat_path: null

# Training configuration
ema_decay: 0.9978
clip_grad: 0.0

# GAN configuration
disc_weight: 0.75
perceptual_weight: 1.0
disc_start_epoch: 8
disc_upd_start_epoch: 6
lpips_start_epoch: 0
max_d_weight: 10000.0
disc_updates: 1
disc_loss_type: hinge
gen_loss_type: vanilla

# Discriminator architecture configuration
disc_arch:
  arch:
    dino_ckpt_path: '/home/project/models/discs/dino_vit_small_patch8_224.pth'
    ks: 9
    norm_type: 'bn'
    using_spec_norm: true
    recipe: 'S_8'
  augment:
    prob: 1.0
    cutout: 0.0

disc_optimizer:
  lr: 2.0e-4
  betas: [0.9, 0.95]
  weight_decay: 0.0

# Discriminator learning rate scheduler (cosine annealing with warmup)
disc_scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: 15  # epochs - warmup_epochs = 16 - 1 = 15
  eta_min: 2.0e-5  # final_lr

# Image configuration
image_size: 224

# Sampling configuration
sample_every: 2500

# Compile model
compile: false

# Optimizer configuration
optimizer:
  lr: 2.0e-4
  betas: [0.9, 0.95]
  weight_decay: 0.0

# Learning rate scheduler (cosine annealing with warmup)
scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: 15  # epochs - warmup_epochs = 16 - 1 = 15
  eta_min: 2.0e-5  # final_lr

# Precision
precision: "fp32"