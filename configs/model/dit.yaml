# DiT Module Configuration for Lightning

_target_: src.models.dit_module.DiTModule

# RAE configuration (encoder is frozen for stage2 training)
rae:
  _target_: src.models.stage1.RAE
  encoder_cls: 'Dinov2withNorm'
  encoder_config_path: 'facebook/dinov2-with-registers-base'
  encoder_input_size: 224
  encoder_params:
    dinov2_path: 'facebook/dinov2-with-registers-base'
    normalize: true
  decoder_config_path: '${paths.root_dir}/configs/decoder/ViTB/config.json'
  pretrained_decoder_path: null  # Path to trained RAE checkpoint from stage1
  noise_tau: 0.0
  reshape_to_2d: true

# DiT Configuration
dit:
  _target_: src.models.stage2.DiTwDDTHead
  input_size: 16  # Latent spatial size (16x16)
  patch_size: 1
  in_channels: 768  # RAE latent channels
  hidden_size: [1152, 2048]  # DiT-XL encoder/decoder hidden sizes
  depth: [28, 2]  # DiT-XL encoder/decoder depth
  num_heads: [16, 16]
  mlp_ratio: 4.0
  class_dropout_prob: 0.1
  num_classes: 1000
  use_qknorm: false
  use_swiglu: true
  use_rope: true
  use_rmsnorm: true
  wo_shift: false
  use_pos_embed: true

# Lightning Module configuration
ema_decay: 0.9995
learning_rate: 2.0e-4
weight_decay: 0.0
betas: [0.9, 0.95]
warmup_steps: 5000
max_steps: 100000
num_classes: 1000
null_label: 1000
latent_size: [768, 16, 16]  # [C, H, W]
compile: false